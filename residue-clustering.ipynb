{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import list of ligands to exclude (list pre-generated based on exclusion criteria)\n",
    "\n",
    "ligs2excl = []\n",
    "\n",
    "with open('ligands-to-exclude.txt','r') as file:\n",
    "    line_list = file.readlines()\n",
    "    for line in line_list:\n",
    "        ligs2excl.append(line.split()[0])\n",
    "        \n",
    "\n",
    "print(len(ligs2excl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total number of residues \n",
    "\n",
    "total_res_dict = pickle.load(open('total_res_dict.p','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filters on PDBspheres data\n",
    "\n",
    "datecut = 'current' # to include all templates currently available: 'current'\n",
    "resolutioncut = 'all' # to include all resolutions: 'all'\n",
    "gdccut = '60' \n",
    "Nccut = '15'\n",
    "N4cut = '4'\n",
    "ligsizecut = '8'\n",
    "clcut = '0'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ligand binding dictionary and calculate percentage of residues that each ligand binds\n",
    "\n",
    "ligand_dict = {}\n",
    "template_dict = {}\n",
    "fracres_dict = {}\n",
    "ligs_leaveout = {}\n",
    "all_ligs_remove = []\n",
    "bind_thresh = 0.333\n",
    "\n",
    "for lig in ligs2excl:\n",
    "    all_ligs_remove.append(lig)\n",
    "\n",
    "    \n",
    "for protnow in ['E','S','ORF3a','nsp12','nsp13','nsp14','nsp15','nsp16','nsp3','nsp5','nsp7','nsp8','nsp9',\\\n",
    "                'nsp1','nsp2','ORF7a','nsp4','nsp10','N','ORF8']:\n",
    "    \n",
    "    rlist = []\n",
    "    llist = []\n",
    "    ligand_dict[protnow] = {}\n",
    "    template_dict[protnow] = {}\n",
    "    \n",
    "    ligs_leaveout[protnow] = []\n",
    "    for lig in ligs2excl:\n",
    "        ligs_leaveout[protnow].append(lig)\n",
    "    \n",
    "\n",
    "    file = open('./CCC.confidence_centroid_contacts.'+Nccut+'_10_'+gdccut+'_'+N4cut+'_'+clcut+'.ligs_'+ligsizecut+'.nCoV.'+datecut+'.res'+resolutioncut,'r')\n",
    "    \n",
    "    line_list = file.readlines()\n",
    "    \n",
    "    for line in line_list:          \n",
    "        # viral protein\n",
    "        if line.split()[0].split('.')[0].split('_')[0]=='nCoV':\n",
    "            protein = line.split()[0].split('.')[0].split('_')[1]\n",
    "            if protein=='Spike':\n",
    "                protein = 'S'\n",
    "        \n",
    "        if protein==protnow:\n",
    "            # ligand\n",
    "            ligand = line.split()[0].split('.')[6]\n",
    "\n",
    "            # residues\n",
    "            binding_residues = line.split()[-1].split(',')\n",
    "            del binding_residues[-1]\n",
    "                \n",
    "            if len(binding_residues)>0:\n",
    "                if ligand not in llist:\n",
    "                    llist.append(ligand)\n",
    "    \n",
    "                for residue in binding_residues:\n",
    "                    if residue not in rlist:\n",
    "                        rlist.append(residue)\n",
    "                        \n",
    "                    if ligand not in ligand_dict[protnow]:\n",
    "                        ligand_dict[protnow][ligand] = [residue]\n",
    "                    elif ligand in ligand_dict[protnow] and residue not in ligand_dict[protnow][ligand]:\n",
    "                        ligand_dict[protnow][ligand].append(residue)\n",
    "                        \n",
    "                    if ligand not in ligs_leaveout[protnow]:\n",
    "                        if residue not in template_dict[protnow]:\n",
    "                            template_dict[protnow][residue] = 1\n",
    "                        elif residue in template_dict[protnow]:\n",
    "                            template_dict[protnow][residue] = template_dict[protnow][residue] + 1\n",
    "                        \n",
    "    file.close()\n",
    "        \n",
    "    rlist.sort()\n",
    "    llist.sort()\n",
    "    \n",
    "    fracres_dict[protnow] = {}\n",
    "    \n",
    "    for lig in llist:\n",
    "        fracres_dict[protnow][lig] = float(len(ligand_dict[protnow][lig]))/float(total_res_dict[protnow])\n",
    "        if fracres_dict[protnow][lig]>bind_thresh and lig not in ligs_leaveout[protnow]:\n",
    "            ligs_leaveout[protnow].append(lig)\n",
    "            \n",
    "    rfd_sorted = sorted(fracres_dict[protnow].items(), key=lambda x: x[1], reverse=True)   \n",
    "    \n",
    "    for lig in ligs_leaveout[protnow]:\n",
    "        if lig not in all_ligs_remove:\n",
    "            all_ligs_remove.append(lig)\n",
    "            \n",
    "            \n",
    "pickle.dump(ligs_leaveout,open('ligs_leaveout.p','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the contact ligand residues into data structures\n",
    "# Filter ligands by SMILES strings and percentage of residues they bind\n",
    "# Filter PDB templates by date available, resolution, GDC value\n",
    "\n",
    "def findOccurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "with open('./CCC.confidence_centroid_contacts.'+Nccut+'_10_'+gdccut+'_'+N4cut+'_'+clcut+'.ligs_'+ligsizecut+'.nCoV.'+datecut+'.res'+resolutioncut) as M:\n",
    "    wer=M.readlines()\n",
    "\n",
    "ncovdict=defaultdict(lambda: ([], [])) #first is conta, second resid\n",
    "    \n",
    "ligdict=defaultdict(set)\n",
    "filedict=defaultdict(set)\n",
    "\n",
    "all_contacts = {}\n",
    "for protnow in ['E','S','ORF3a','nsp12','nsp13','nsp14','nsp15','nsp16','nsp3','nsp5','nsp7','nsp8','nsp9',\\\n",
    "                'nsp1','nsp2','ORF7a','nsp4','nsp10','N','ORF8']:\n",
    "    all_contacts[protnow] = 0\n",
    "\n",
    "for lin in wer:\n",
    "    if lin.split()[0].split('.')[0].split('_')[0]=='nCoV':\n",
    "        ligand = lin.split()[0].split('.')[6]\n",
    "        ncovfind=lin.find('nCoV_')\n",
    "        underfind=findOccurrences(lin,'_')\n",
    "        virprot=lin[(underfind[min(k for k,x in enumerate(underfind) if x>ncovfind)]+1):underfind[min(k for k,x in enumerate(underfind) if x>ncovfind)+1]]\n",
    "        if virprot=='Spike':\n",
    "            virprot='S'     \n",
    "        if ligand not in ligs_leaveout[virprot]:  \n",
    "            fins=findOccurrences(lin, '.')\n",
    "            spherfind=lin.find('.Sphere.')\n",
    "            ligid=lin[(fins[min(k for k,x in enumerate(fins) if x>spherfind)]+1):fins[1+min(k for k,x in enumerate(fins) if x>spherfind)]]\n",
    "            contstr=lin.strip(',\\n').split()[-1]\n",
    "            conts=contstr.split(',')\n",
    "            all_contacts[virprot] = all_contacts[virprot] + len(conts)\n",
    "            fileSrc=lin[:lin.find(':')]\n",
    "            nonodes=[1 if (not cont[-2]=='_') else 0 for cont in conts]\n",
    "            if any(nonodes):\n",
    "                continue\n",
    "            ncovdict[virprot][1].extend(conts)\n",
    "            for cont in conts:\n",
    "                ligdict[virprot+'.'+cont].add(ligid)\n",
    "                filedict[virprot+'.'+cont].add(fileSrc)\n",
    "            for pair in itertools.combinations(conts,2):\n",
    "                ncovdict[virprot][0].append(pair)\n",
    "\n",
    "print(all_contacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into weighted networkx graphs, one for each viral protein\n",
    "\n",
    "from networkx.algorithms import community\n",
    "import numpy as np\n",
    "\n",
    "H_all=dict()\n",
    "shared_int_dict=dict()\n",
    "all_int_dict=dict()\n",
    "nnodes_dict=dict()\n",
    "\n",
    "# Loop over the viral proteins\n",
    "# Create a graph for each protein\n",
    "# Loop over all residues that contact ligands\n",
    "# Add the residues as nodes\n",
    "# Store the ligands they contact in a list (as well as files to find them)\n",
    "\n",
    "for protnow in ['E','S','ORF3a','nsp12','nsp13','nsp14','nsp15','nsp16','nsp3','nsp5','nsp7','nsp8','nsp9']:\n",
    "    shared_int_dict[protnow]=dict()\n",
    "    all_int_dict[protnow]=dict()\n",
    "    H=nx.Graph()\n",
    "    resa=Counter(ncovdict[protnow][1])\n",
    "    for ress in resa.most_common():\n",
    "        all_int_dict[protnow][ress[0]]=ress[1]\n",
    "        H.add_node(ress[0],contacts=ress[1],ligands=ligdict[protnow+'.'+ress[0]],files=filedict[protnow+'.'+ress[0]])\n",
    "    counta=Counter(ncovdict[protnow][0])\n",
    "    for conn in counta.most_common():\n",
    "        shared_int_dict[protnow][conn[0]]=conn[1]\n",
    "        H.add_edge(conn[0][0], conn[0][1], weight=conn[1], invweight=1.0/conn[1]) #np.exp(-float(conn[1])))\n",
    "    H_all[protnow]=H\n",
    "    nnodes_dict[protnow]=H.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of contacts per residue \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import poisson, gamma, expon, linregress\n",
    "from scipy.optimize import curve_fit\n",
    "from math import exp\n",
    "import pickle\n",
    "\n",
    "cntctsperres = {}\n",
    "area = {}\n",
    "histcounts = {}\n",
    "num_contacts = {}\n",
    "\n",
    "for protnow in ['nsp3','nsp5','nsp12','S','ORF3a','nsp13','nsp14','nsp15','nsp16','nsp9']: \n",
    "    num_contacts[protnow] = {}\n",
    "    cntctsperres[protnow] = []\n",
    "    contributes=nx.get_node_attributes(H_all[protnow],'contacts')\n",
    "    for nd in H_all[protnow]:\n",
    "        cntctsperres[protnow].append(contributes[nd])\n",
    "        if nd not in num_contacts[protnow]:\n",
    "            num_contacts[protnow][nd] = contributes[nd]\n",
    "    \n",
    "    plt.figure()\n",
    "    histout = plt.hist(cntctsperres[protnow],bins=6500,range=(1,6500))\n",
    "    plt.title(protnow)\n",
    "    plt.xlabel('Contacts per residue')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    bincounts = histout[0]\n",
    "    histcounts[protnow] = bincounts\n",
    "    area[protnow] = sum(bincounts)\n",
    "    binedges = histout[1]\n",
    "    bincenters = 0.5 * (binedges[1:] + binedges[:-1])\n",
    "    \n",
    "    print('mean = '+str(np.mean(cntctsperres[protnow])))\n",
    "    print('std = '+str(np.std(cntctsperres[protnow])))\n",
    "    print('total = '+str(sum(cntctsperres[protnow])))\n",
    "    print('total area = '+str(area[protnow]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of contacts per residue (continued)\n",
    "\n",
    "plt.figure()\n",
    "colors = ['r','b','m','c','g']\n",
    "labels = []\n",
    "i = 0\n",
    "\n",
    "sars2_normalized_counts_dict = {}\n",
    "\n",
    "for protnow in ['S','nsp3','nsp5','nsp9','nsp12','nsp13','nsp14','nsp16','nsp15','ORF3a']:\n",
    "    normalized_counts = [float(num)/float(area[protnow]) for num in histcounts[protnow]]\n",
    "    sars2_normalized_counts_dict[protnow] = normalized_counts\n",
    "    labels.append(protnow)\n",
    "    plt.scatter(np.log(np.arange(1,len(normalized_counts)+1,1)),np.log(normalized_counts))\n",
    "    \n",
    "    \n",
    "plt.xlabel('Log(Contacts per residue)',fontsize=16)\n",
    "plt.ylabel('Log(Normalized count)',fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(labels,fontsize=14,loc=(1.05,0.02))\n",
    "        \n",
    "#plt.savefig('figures/contacts_per_residue_plot_SARS2proteins.png')        \n",
    "#pickle.dump(sars2_normalized_counts_dict,open('sars2_normalized_counts_dict.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find min and max number of residues in contact across all ligands\n",
    "\n",
    "def res_contacts(prtn,filename):\n",
    "\n",
    "    min_res_contact = 100\n",
    "    max_res_contact = 0\n",
    "    \n",
    "    file = open(filename,'r')\n",
    "    line_list = file.readlines()\n",
    "\n",
    "    for line in line_list:\n",
    "        # viral protein\n",
    "        if line.split()[0].split('.')[0].split('_')[0]=='nCoV':\n",
    "            protein = line.split()[0].split('.')[0].split('_')[1]\n",
    "            if protein=='Spike':\n",
    "                protein = 'S'\n",
    "                    \n",
    "        if protein==prtn:\n",
    "            N4 = int(line.split()[11])\n",
    "            if N4 < min_res_contact and N4 > 0:\n",
    "                min_res_contact = N4\n",
    "            if N4 > max_res_contact:\n",
    "                max_res_contact = N4\n",
    "            \n",
    "    file.close()\n",
    "\n",
    "    return min_res_contact, max_res_contact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import shortest_paths\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "from scipy import cluster\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find max value across series of lists\n",
    "\n",
    "def max_nested(list_of_lists):\n",
    "    return max([max(x) for x in list_of_lists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut tree at specific height and find relevant clusters - threshold based on number of contacts\n",
    "\n",
    "def cut_res_clust(protnow,comout,ordered_list_of_res,cut_height,H_all,cluster_ligand_dict,min_cp_size,max_cp_size,all_contacts):\n",
    "    cutree = cluster.hierarchy.cut_tree(comout,height=cut_height)\n",
    "    clusout=[(x,cutree[k][0]) for k,x in enumerate(ordered_list_of_res)]\n",
    "    clustall=[]\n",
    "    for k in range(max([x[1] for x in clusout])+1):\n",
    "        clustall.append([x[0] for x in clusout if x[1]==k])\n",
    "    contributes=nx.get_node_attributes(H_all[protnow],'contacts')\n",
    "    ligributes=nx.get_node_attributes(H_all[protnow],'ligands')\n",
    "    n_clusters=len(clustall)\n",
    "    cplist=[]\n",
    "    cluster_ligand_dict[cut_height]={}\n",
    "    for m,clust in enumerate(clustall):\n",
    "        totalContacts=(sum([contributes[res] for res in clust]))\n",
    "        contactsperres = totalContacts/float(len(clust))\n",
    "        cout=Counter(itertools.chain.from_iterable([list(ligributes[res]) for res in clust]))\n",
    "        commall=cout.most_common()\n",
    "        commadj=list()\n",
    "        for ite in commall:\n",
    "            commadj.append((ite[0],ite[1],len(clust),float(ite[1])/len(clust)))\n",
    "        commadj.sort(key=lambda tup: -tup[3])\n",
    "        cpnow=dict()\n",
    "        cpnow['index']=m\n",
    "        cluster_ligand_list=[]\n",
    "       \n",
    "        if contactsperres>90 and len(clust)>(min_cp_size-1) and len(clust)<(max_cp_size+1):\n",
    "            cpnow['totalContacts']=totalContacts\n",
    "            cpnow['residuesList']=clust\n",
    "            cplist.append(clust)\n",
    "            mols=[]\n",
    "            proteincount=0\n",
    "            proteinset=set()\n",
    "            for r in range(len(commadj)):\n",
    "                if commadj[r][3]<0.75:\n",
    "                    break\n",
    "                x=commadj[r]\n",
    "                cluster_ligand_list.append((x[0],x[3]))\n",
    "                if len(x[0])==3:\n",
    "                    continue\n",
    "                else:\n",
    "                    proteinset.add(x[0])\n",
    "                    proteincount+=1\n",
    "            cluster_ligand_dict[cut_height][m]=(len(clust),cluster_ligand_list)\n",
    "            cpnow['proteinCount']=proteincount\n",
    "            ligroups=[]\n",
    "            if proteincount:\n",
    "                ligroups.append({'ligs':[x for x in proteinset]})\n",
    "            disty=[]\n",
    "            if not disty:\n",
    "                cpnow['ligroups']=ligroups\n",
    "                cpnow['ligroups'].append({'ligs':[x[0] for x in mols]})\n",
    "                continue\n",
    "            comout2=linkage(disty,method='complete',optimal_ordering=True)\n",
    "            cutree2 = cluster.hierarchy.cut_tree(comout2,height=0.1251)\n",
    "            for groupy in range(max_nested(cutree2)+1):\n",
    "                indsnow=[k for k,val in enumerate(cutree2) if val[0]==groupy]\n",
    "                molgroup=[]\n",
    "                for ind in indsnow:\n",
    "                    molgroup.append(mols[ind][1])  \n",
    "                print('-----')\n",
    "        \n",
    "    return cplist, cluster_ligand_dict, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unique clusters - bottom to top of dendrogram (largest unique within size range)\n",
    "\n",
    "def unique_clusters(qvec,cpdict):\n",
    "    cpfinal = []\n",
    "\n",
    "    for q in qvec:\n",
    "        if len(cpdict[q])>0:\n",
    "            for check_cluster in cpdict[q]:\n",
    "                if len(cpfinal)>0:\n",
    "                    unique=1\n",
    "                    cpfinal_add = []\n",
    "                    for final_cluster in cpfinal:\n",
    "                        if set(final_cluster).issubset(set(check_cluster))==True or set(final_cluster)==set(check_cluster):\n",
    "                            cpfinal.remove(final_cluster)\n",
    "                            if check_cluster not in cpfinal_add and check_cluster not in cpfinal:\n",
    "                                cpfinal_add.append(check_cluster)\n",
    "                            unique=0\n",
    "                        \n",
    "                    if len(cpfinal_add)>0:\n",
    "                        cpfinal.extend(cpfinal_add)   \n",
    "                    \n",
    "                    if unique==1 and check_cluster not in cpfinal: \n",
    "                        cpfinal.append(check_cluster)\n",
    "                elif len(cpfinal)==0:\n",
    "                    cpfinal.append(check_cluster)\n",
    "    \n",
    "    return cpfinal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final clusters to output file\n",
    "\n",
    "def save_final_clusters(prtn,final_clusters,directory): \n",
    "\n",
    "    try:\n",
    "        os.system('mkdir '+directory)\n",
    "    except:\n",
    "        pass\n",
    "    f = open(directory+'/clusters_'+prtn+'.txt','w')\n",
    "    for i in range(1,len(final_clusters)+1):\n",
    "        f.write('%d:\\t' % i)\n",
    "        for item in final_clusters[i-1]:\n",
    "            f.write(str(item)+',')     \n",
    "        f.write('\\n')   \n",
    "    f.close()\n",
    "    \n",
    "    f = open(directory+'/clusters_'+prtn+'_formatted.txt','w')\n",
    "    for i in range(1,len(final_clusters)+1):\n",
    "        f.write('%d:\\t' % i)\n",
    "        for item in final_clusters[i-1]:\n",
    "            f.write(str(item[0:-2])+', ')     \n",
    "        f.write('\\n')   \n",
    "    f.close()\n",
    "    \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renumber clusters for current date for nsp12 and Spike\n",
    "\n",
    "def renumber_final_clusters(prtn,final_clusters_temp): \n",
    "    \n",
    "    if prtn=='nsp12':\n",
    "        final_clusters_renum = []\n",
    "        final_clusters_renum.append(final_clusters_temp[3])\n",
    "        final_clusters_renum.extend(final_clusters_temp[0:3])\n",
    "    elif prtn=='S':\n",
    "        final_clusters_renum = []\n",
    "        final_clusters_renum.extend(final_clusters_temp[0:2])\n",
    "        final_clusters_renum.append(final_clusters_temp[3])\n",
    "        final_clusters_renum.append(final_clusters_temp[2])\n",
    "        final_clusters_renum.extend(final_clusters_temp[4:])\n",
    "    \n",
    "    return final_clusters_renum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary with keys = residues, values = ligands they bind\n",
    "# ligands filtered by SMILES string and percentage of residues they bind\n",
    "\n",
    "def reslig_dict(prtn,filename):\n",
    "\n",
    "    residue_list = []\n",
    "    ligand_list = []\n",
    "    res_lig_dict = {}\n",
    "    \n",
    "    file = open(filename,'r')\n",
    "    line_list = file.readlines()\n",
    "\n",
    "    for line in line_list:        \n",
    "        # viral protein\n",
    "        if line.split()[0].split('.')[0].split('_')[0]=='nCoV':\n",
    "            protein = line.split()[0].split('.')[0].split('_')[1]\n",
    "            if protein=='Spike':\n",
    "                protein = 'S'\n",
    "                    \n",
    "        if protein==prtn:\n",
    "            # ligand\n",
    "            ligand = line.split()[0].split('.')[6]\n",
    "            \n",
    "            if ligand not in ligs_leaveout[prtn]: \n",
    "            #if ligand not in all_ligs_remove:\n",
    "                if ligand not in ligand_list:\n",
    "                    ligand_list.append(ligand)\n",
    "\n",
    "                # residues\n",
    "                binding_residues = line.split()[-1].split(',')\n",
    "                del binding_residues[-1]\n",
    "    \n",
    "                for residue in binding_residues:\n",
    "                    if residue not in residue_list:\n",
    "                        residue_list.append(residue)\n",
    "                    if residue in res_lig_dict:\n",
    "                        if ligand not in res_lig_dict[residue]:\n",
    "                            res_lig_dict[residue].append(ligand)\n",
    "                    elif residue not in res_lig_dict:\n",
    "                        res_lig_dict[residue] = [ligand]\n",
    "            \n",
    "    file.close()\n",
    "        \n",
    "    residue_list.sort()\n",
    "    ligand_list.sort()\n",
    "\n",
    "    return res_lig_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary with ligands that bind to residues in each cluster\n",
    "\n",
    "def cluster_dict(final_clusters,reslig_dict):\n",
    "    cluster_dict = {}\n",
    "    \n",
    "    clind = 1\n",
    "    for clust in final_clusters:  \n",
    "        cluster_dict[clind] = {}\n",
    "        cluster_dict[clind]['residues'] = clust\n",
    "        cluster_ligand_list = []\n",
    "        \n",
    "        for res in clust:\n",
    "            n_res = 0\n",
    "            \n",
    "            for lig in reslig_dict[str(res)]:\n",
    "                n_present = 0\n",
    "                liginlist = 0\n",
    "                # calculate fraction of residues in cluster to which ligand binds\n",
    "                for res1 in clust:\n",
    "                    if lig in reslig_dict[str(res1)]:\n",
    "                        n_present = n_present + 1\n",
    "                                \n",
    "                lig_present_frac = float(n_present)/float(len(clust))\n",
    "                \n",
    "                if ((lig,lig_present_frac)) not in cluster_ligand_list:\n",
    "                    cluster_ligand_list.append((lig,lig_present_frac))\n",
    "                 \n",
    "        cluster_ligand_list_sorted = sorted(cluster_ligand_list, key=lambda x: x[1], reverse=True)\n",
    "        cluster_dict[clind]['ligands'] = cluster_ligand_list_sorted\n",
    "        clind = clind+1\n",
    "    \n",
    "    return cluster_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final ligands to output file\n",
    "# Filter ligands by SMILES strings and percentage of residues they bind\n",
    "\n",
    "def save_final_ligands(prtn,cluster_dict,directory): \n",
    "\n",
    "    try:\n",
    "        os.system('mkdir '+directory)\n",
    "    except:\n",
    "        pass\n",
    "    f = open(directory+'/ligands_'+prtn+'.txt','w')\n",
    "    for key,value in cluster_dict.items():\n",
    "        f.write('%d:\\t' % key)\n",
    "        for item in value['ligands']:\n",
    "            if item[0] not in ligs_leaveout[prtn]:\n",
    "                f.write(str(item)+',')     \n",
    "        f.write('\\n')   \n",
    "    f.close()\n",
    "    \n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of consensus cluster info\n",
    "# Filter ligands by SMILES strings and percentage of residues they bind\n",
    "\n",
    "def pocket_info_plot(prot,clust_dict,lig_bound_frac):\n",
    "    clsize = []\n",
    "    nligs = []\n",
    "    labels = []\n",
    "    \n",
    "    for key,value in clust_dict.items():\n",
    "        clsize.append(len(clust_dict[key]['residues']))\n",
    "        labels.append(str(key))\n",
    "        \n",
    "        lig_count = []\n",
    "        for lig in clust_dict[key]['ligands']:\n",
    "            if lig[1]>=lig_bound_frac and lig[0] not in ligs_leaveout[prot]:\n",
    "                lig_count.append(lig[0])\n",
    "                \n",
    "        nligs.append(len(lig_count))\n",
    "\n",
    "    if len(labels)<13:\n",
    "        x = 1.25*np.arange(1,13+1)\n",
    "        diff = 13-len(labels)\n",
    "        for i in range(0,diff):\n",
    "            clsize.append(0)\n",
    "            nligs.append(0)\n",
    "    elif len(labels)==13:\n",
    "        x = 1.25*np.arange(1,len(labels)+1)\n",
    "    \n",
    "    width = 0.4  # width of bars\n",
    " \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "    ax2 = ax.twinx() # create another set of axes that shares the same x-axis as ax\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, clsize, width, label='Residues', color='tab:blue')\n",
    "    rects2 = ax2.bar(x + width/2, nligs, width, label='Ligands', color='tab:orange')\n",
    "\n",
    "    ax.set_ylabel('Number of residues',fontsize=15)\n",
    "    ax.set_ylim([0,45])\n",
    "    ax2.set_ylabel('Number of ligands',fontsize=15)\n",
    "    ax2.set_ylim([0,250])\n",
    "    ax.yaxis.label.set_color('tab:blue')\n",
    "    ax2.yaxis.label.set_color('tab:orange')\n",
    "    ax.spines['left'].set_color('tab:blue')\n",
    "    ax2.spines['right'].set_color('tab:orange')\n",
    "    ax.tick_params(axis='y', colors='tab:blue')\n",
    "    ax2.tick_params(axis='y', colors='tab:orange')\n",
    "    ax.set_xlabel('Pocket',fontsize=15)\n",
    "    ax.set_title(prot,fontsize=15)  \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,fontsize=13)\n",
    "    ax.set_yticks([0,5,10,15,20,25,30,35,40,45])\n",
    "    ax.set_yticklabels(['0','5','10','15','20','25','30','35','40','45'],fontsize=13)\n",
    "    ax2.set_yticks([0,25,50,75,100,125,150,175,200,225,250])\n",
    "    ax2.set_yticklabels(['0','25','50','75','100','125','150','175','200','225','250'],fontsize=13)\n",
    "    plt.show()\n",
    "    #plt.savefig('figures/residue_clusters_bar_chart_'+prot+'.png')\n",
    "\n",
    "       \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering on viral proteins \n",
    "\n",
    "maindirectory = 'cluster-output-ncov-residues-shortestpath-CCC-'+Nccut+'-10-'+gdccut+'-'+N4cut+'-'+clcut+'.ligs_'+ligsizecut\n",
    "\n",
    "try:\n",
    "    os.system('mkdir '+maindirectory)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "directory = 'cluster-output-ncov-residues-shortestpath-CCC-'+Nccut+'-10-'+gdccut+'-'+N4cut+'-'+clcut+'.ligs_'+ligsizecut+'/date_'+datecut+'_res'+resolutioncut\n",
    "\n",
    "filename = './CCC.confidence_centroid_contacts.'+Nccut+'_10_'+gdccut+'_'+N4cut+'_'+clcut+'.ligs_'+ligsizecut+'.nCoV.'+datecut+'.res'+resolutioncut\n",
    "\n",
    "cpall=defaultdict(list)\n",
    "cldict={}\n",
    "for protnow in ['nsp12','S','nsp5','nsp3','ORF3a','nsp13','nsp14','nsp15','nsp16','nsp9']:\n",
    "    print(protnow)\n",
    "    Q=dict(shortest_paths.shortest_path_length(H_all[protnow],weight='invweight'))\n",
    "    contall=nx.get_node_attributes(H_all[protnow],'contacts')\n",
    "    cont_thresh=1\n",
    "    ordered_list_of_res=sorted([x for x in Q.keys() if contall[x]>cont_thresh],key=lambda qk: int(qk[1:-2]))\n",
    "    pdistmat=[]\n",
    "    for k,res in enumerate(ordered_list_of_res):\n",
    "        pdistmat.extend([Q[res][ordered_list_of_res[x]] if ordered_list_of_res[x] in Q[res] else 1 for x in range(k+1,len(ordered_list_of_res))])\n",
    "    \n",
    "    try:\n",
    "        comout=linkage(pdistmat,method='complete',optimal_ordering=True)\n",
    "        \n",
    "        #plt.figure(figsize=(100,25))\n",
    "        #dendrogram(comout,labels=ordered_list_of_res,leaf_font_size=10)\n",
    "        #plt.title(protnow)\n",
    "    \n",
    "        mrc = res_contacts(protnow,filename)\n",
    "        min_cp_size=10\n",
    "        \n",
    "        max_cp_size=mrc[1]\n",
    "        \n",
    "        print('min',mrc[0],'max',mrc[1])\n",
    "        cluster_ligand_dict={}\n",
    "        cpdict={}\n",
    "        n_clusters=100\n",
    "        q=0.001\n",
    "        qvec = []\n",
    "\n",
    "        while n_clusters > 1:\n",
    "            crc=cut_res_clust(protnow,comout,ordered_list_of_res,q,H_all,cluster_ligand_dict,min_cp_size,max_cp_size,all_contacts)\n",
    "            cplist=crc[0]\n",
    "            cpdict[q]=crc[0]\n",
    "            cpall[protnow].append(cplist)\n",
    "            cluster_ligand_dict=crc[1]\n",
    "            n_clusters=crc[2]\n",
    "            qvec.append(q)\n",
    "            q=q+0.001\n",
    "        del qvec[-1]\n",
    "    \n",
    "        final_clusters_temp=unique_clusters(qvec,cpdict)\n",
    "        \n",
    "        if datecut=='current' and (protnow=='nsp12' or protnow=='S'):\n",
    "            final_clusters = renumber_final_clusters(protnow,final_clusters_temp)\n",
    "        else:\n",
    "            final_clusters = final_clusters_temp\n",
    "                    \n",
    "        save_final_clusters(protnow,final_clusters,directory)\n",
    "    \n",
    "        rldict = reslig_dict(protnow,filename)\n",
    "        cldict[protnow] = cluster_dict(final_clusters,rldict)\n",
    "    \n",
    "        save_final_ligands(protnow,cldict[protnow],directory)\n",
    "        lig_bound_frac = 0.5\n",
    "        pocket_info_plot(protnow,cldict[protnow],lig_bound_frac)\n",
    "        \n",
    "        pickle.dump(cldict,open(directory+'/cldict.p', 'wb'))\n",
    "    \n",
    "    except ValueError:\n",
    "        print('Empty distance matrix')\n",
    "    \n",
    "    print('---:::::---:::::::---')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-venv",
   "language": "python",
   "name": "python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
